---
title: '以多數決模型來辨別交通號誌'
publishedAt: '2024-09-16'
summary: 'The post shows how one might use data augumenation to fit a machine-learning model.'
---


# GTSRB
[GTSRB](https://benchmark.ini.rub.de/gtsrb_dataset.html#Downloads)<sup>*1</sup> 資料集收集43種德國交通號誌。資料集將圖檔分類為43個資料夾，資料夾以五個數字命名，自00000到00042。每一個資料夾儲存一種交通號誌<sup>*2</sup>。每個資料夾包含了多個不同明亮，模糊程度的圖檔(Data augumenation)<sup>*3</sup>。 


```
#  [1] "00000" "00001" "00002" "00003" "00004" "00005" "00006" "00007" "00008"
# [10] "00009" "00010" "00011" "00012" "00013" "00014" "00015" "00016" "00017"
# [19] "00018" "00019" "00020" "00021" "00022" "00023" "00024" "00025" "00026"
# [28] "00027" "00028" "00029" "00030" "00031" "00032" "00033" "00034" "00035"
# [37] "00036" "00037" "00038" "00039" "00040" "00041" "00042"

#   0    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15 
#  150 1500 1500  960 1320 1260  300  960  960  990 1350  900 1410 1440  540  420 
#   16   17   18   19   20   21   22   23   24   25   26   27   28   29   30   31 
#  300  750  810  150  240  240  270  360  180 1020  420  180  360  180  300  540 
#   32   33   34   35   36   37   38   39   40   41   42 
#  180  480  300  810  270  150 1380  210  240  180  180 

```
![Traffic_signs](/traffic_signs.png)

準確地辨識這些號誌對於車用號誌辦識系統將有很大助益，這裡我們展示[前文](https://yslin.me/blog/QMNIST)曾提到ensemble的概念，使用三個深層[CNN模型](https://github.com/kkweon/mnist-competition)VggNet16, ResNet and VggNet5來進行此分類作業。

1. 我們先預處理資料，以便使用Tensorflow套件處理。

```
python3 preprocess.py "../data/GTSRB/GTSRB-Training_fixed" "tmp_data/GTSRB.pkl" --random_seed 42
```

2. 接下來我們使用稍微修改[卷積類神經網絡辨識MNIST影像](https://yslin.me/blog/CNN2)模型訓練碼，將模型置換為ResNet，VggNet或VggNet5。

```
python3 fit_resnet.py 'tmp_data/GTSRB.pkl' "models/resnet_history.pkl" "models/resnet.weights.h5" "models/resnet.keras" --nepoch 30

python3 fit_vgg5.py 'tmp_data/GTSRB.pkl' "models/vgg5_history.pkl" "models/vgg5.weights.h5" "models/vgg5.keras" --nepoch 30

python3 fit_vgg16.py 'tmp_data/GTSRB.pkl' "models/vgg16_history.pkl" "models/vgg16.weights.h5" "models/vgg16.keras" --nepoch 30
```

3. 在此我們僅繪製驗證組的訓練結果，因為我們觀注的是模型可否應用在它未遭遇到的資料。

![Model_performance](/ThreeNets_GTSRB_val.png)

三個模型在經過30個遞迴後，在測試組的評估都達到相當高的正確率。

<Table data={{
  headers: ["Model", "accuracy", "loss","errors"],
  rows: [
    ["VggNet16", "0.9976", "0.0120", "16"],
    ["VggNet5", "0.9941", "0.0264", "39"],
    ["ResNet", "0.9899", "0.0438", "67"],
    ["Ensemble", "0.9980", "NA", "13"],
  ]
}} />

在6660筆的號誌圖檔中，三個模型的誤判數分別為16, 39, 67。


4. 接下來我們計算三個模型對測試組的推測，並以多數決來決定最終的預測類別。

```
vgg16_prob = vgg16_model.predict(X_test)
vgg5_prob = vgg5_model.predict(X_test)
resnet_prob = resnet_model.predict(X_test)
vgg16_pred = np.array([np.argmax(i) for i in vgg16_prob])
vgg5_pred = np.array([np.argmax(i) for i in vgg5_prob])
resnet_pred = np.array([np.argmax(i) for i in resnet_prob])

# Stack predictions
predictions = np.vstack((vgg16_pred, vgg5_pred, resnet_pred))

# Find the majority vote for each sample (column)
majority_vote = mode(predictions, axis=0)
```

多數決模型僅誤判了13張圖檔，達到了99.8%的正確率。下圖印出此13張誤判的圖檔，圖上的標籤顯示，正確類別/預測類別。


![Ensemble_error](/ensemble_error.png)


# 註腳
1. GTSRB (German Traffic Sign Recognition Benchmark (GTSRB) Dataset)
2. 資料的來源網站有多個不同zip檔案，這兒此指的是GTSRB-Training_fixed.zip檔案。
3. 資料增強(Data augumentation)，是一統計學術語借用，機器學習指稱些微不同的概念。在知覺心理學中，雖然同一個外界物體的影像，在不同的光照條件下，其物理影像呈現不同，在當觀察者心理上，會將之感知為同一個物體。在機器學習中，資料增強指的是，我們使用軟體，例如Photoshop或是Python script改變同一個影像檔的明暗、模糊程度或是改變環境的光照方式，使同張照片, 同一物體，外觀改變，但僅止於色彩，明暗或是擺放角度等等，實際照片中的內容不變。如此在進行訓練時，資料樣本總數會明顯地增加。

# 附錄
## 1. 繪製交通號誌的程式碼
```
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import csv
import random


images = [] # images
labels = [] # corresponding labels
data_path = '../data/GTSRB/GTSRB-Training_fixed'
ppm_files = []

# 0, 1, ... 42  // loop over all 42 classes
for c in range(0, 43):

    # subdirectory for class
    prefix = data_path + '/' + format(c, '05d') + '/' 
    # annotations file
    gtFile = open(prefix + 'GT-'+ format(c, '05d') + '.csv') 
    
    # csv parser for annotations file
    gtReader = csv.reader(gtFile, delimiter=';') 
    next(gtReader) # skip header

    # loop over all images in current annotations file
    for row in gtReader:
        # the 1th column is the filename
        ppm_files.append(prefix + row[0])
        images.append(plt.imread(prefix + row[0])) 
        labels.append(row[7]) # the 8th column is the label
    gtFile.close()


PROJECT_ROOT_DIR = "."
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images")
os.makedirs(IMAGES_PATH, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)

nsample = len(images)
# Generate 50 unique random numbers from 0 to 26639
random_index = random.sample(range(nsample), 50)

n_rows = 3
n_cols = 3
plt.figure(figsize=(n_cols * 2, n_rows * 2))
for row in range(n_rows):
    for col in range(n_cols):
        index  = n_cols * row + col
        rindex = random_index[n_cols * row + col]
        image = images[rindex]
        label = labels[rindex]
        plt.subplot(n_rows, n_cols, index + 1)
        plt.imshow(image, cmap="binary", interpolation="nearest")
        plt.axis('off')
        plt.title(label, fontsize=20)
        
plt.subplots_adjust(wspace=0.2, hspace=0.5)
plt.tight_layout()
save_fig('traffic_signs')
```

## 2. 預處理資料的程式碼
```
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import csv
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
import pickle as pkl
import argparse


# Desired image size
IMG_HEIGHT = 32
IMG_WIDTH = 32


# Function to load and preprocess images
def load_and_preprocess_image(path):
    image = Image.open(path)
    image = image.resize((IMG_HEIGHT, IMG_WIDTH))
    image = np.array(image) / 255
    return image


if __name__ == "__main__":
    # python3 preprocess.py "../data/GTSRB/GTSRB-Training_fixed" "tmp_data/GTSRB.pkl" --random_seed 42

    parser = argparse.ArgumentParser(description="Process GTSRB data")

    parser.add_argument("data_path", type=str, help="Path to the raw data")
    parser.add_argument("output_path", type=str, help="Path to the output")
    parser.add_argument("--random_seed", type=int, default=42)

    args = parser.parse_args()
    DATA_PATH = args.data_path
    OUTPUT_DIR = "tmp_data/GTSRB.pkl"
    random_seed = args.random_seed


    labels, ppm_files = [], []  
    for c in range(0, 43):

        prefix = DATA_PATH + "/" + format(c, "05d") + "/"
        gtFile = open(prefix + "GT-" + format(c, "05d") + ".csv")

        gtReader = csv.reader(gtFile, delimiter=";")
        next(gtReader)  # skip header

        for row in gtReader:
            # the 1th column is the filename
            ppm_files.append(prefix + row[0])
            labels.append(row[7])  # the 8th column is the label
        gtFile.close()

    images = np.array([load_and_preprocess_image(f) for f in ppm_files])
    labels = np.array([int(label) for label in labels])
    nclass = len(np.unique(labels))

    X_train_full, X_test, y_train_full, y_test = train_test_split(
        images, labels, random_state=random_seed)

    X_train_full, y_train_full = shuffle(X_train_full, y_train_full,random_state=random_seed)

    X_train, X_valid, y_train, y_valid = train_test_split(
        X_train_full, y_train_full, random_state=random_seed)

    data = {
        "X_train": X_train,
        "X_valid": X_valid,
        "X_test": X_test,
        "y_train": y_train,
        "y_valid": y_valid,
        "y_test": y_test,
    }


    with open(OUTPUT_DIR, "wb") as file_obj:
        pkl.dump(data, file_obj)

```

## 3. ResNet訓練碼

VggNet和VggNet5與此程式碼類似，僅須置換建模部份即可

```
import pickle as pkl
import os
import numpy as np

import argparse
from tensorflow import keras

from utils.resnet import resnet as resnet50

import tensorflow as tf
from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Nadam


if __name__ == "__main__":
    # python3 fit_resnet.py 'tmp_data/GTSRB.pkl' "models/resnet_history.pkl" "models/resnet.weights.h5" "models/resnet.keras" --nepoch 30
    
    parser = argparse.ArgumentParser(description="Process GTSRB data")

    parser.add_argument("data_path", type=str, help="Path to the data file")
    parser.add_argument("output_path", type=str, help="Path to the history file")
    parser.add_argument("weight_output", type=str, help="Path to the model weights")
    parser.add_argument("model_output", type=str, help="Path to the model")
    parser.add_argument("--nepoch", type=int, default=30, help="Number of epoch")

    args = parser.parse_args()
    DATA_PATH = args.data_path
    HISTORY_PATH = args.output_path

    WEIGHT_PATH = args.weight_output
    MODEL_PATH = args.model_output

    nepoch = args.nepoch

    print(f"Data path provided: {DATA_PATH}")
    print(f"Data were stored: {HISTORY_PATH}")
    print(f"Weight was stored: {WEIGHT_PATH}")
    print(f"Model was stored: {MODEL_PATH}")
    print(f"The number of epoch: {nepoch}")

    with open(DATA_PATH, 'rb') as f:
        data = pkl.load(f)

    X_train = data['X_train']
    X_valid = data['X_valid']
    X_test = data['X_test']
    y_train = data['y_train']
    y_valid = data['y_valid']
    y_test = data['y_test']

    nclass = len(np.unique(y_train))
    input_shape = X_train[0].shape

    X = Input(shape=input_shape)
    y = resnet50(X, nclass)
    model = Model(X, y, name="resnet")


    model.compile(
        loss="sparse_categorical_crossentropy",
        optimizer=Nadam(learning_rate=0.001), 
        metrics=["accuracy"])

    cp_callback = tf.keras.callbacks.ModelCheckpoint(
        filepath=WEIGHT_PATH, save_weights_only=True, verbose=1)


    history = model.fit(
        X_train, y_train,
        epochs=nepoch,
        batch_size = 32,
        validation_data=(X_valid, y_valid),
        callbacks=[cp_callback])


    print("Evaluating the test set:\n")
    model.evaluate(X_test, y_test)

    with open(HISTORY_PATH, "wb") as file_obj:
        pkl.dump(history, file_obj)

    model.save(MODEL_PATH)

```

## 4. 誤判圖之繪圖碼
```

import numpy as np
from scipy.stats import mode


# Stack predictions
predictions = np.vstack((vgg16_pred, vgg5_pred, resnet_pred))

# Find the majority vote for each sample (column)
majority_vote = mode(predictions, axis=0)[0]

vgg16_incorrect_indices = np.where(vgg16_pred != y_test)[0]
vgg5_incorrect_indices = np.where(vgg5_pred != y_test)[0]
resnet_incorrect_indices = np.where(resnet_pred != y_test)[0]
majority_vote_incorrect_indices = np.where(majority_vote != y_test)[0]

nincorrect = len(majority_vote_incorrect_indices)

n_rows = 4
n_cols = 4
plt.figure(figsize=(n_cols * 2, n_rows * 2))
for row in range(n_rows):
    for col in range(n_cols):
        index = n_cols * row + col
        if index > (nincorrect - 1):
            next
        else:
            incorrect_index = majority_vote_incorrect_indices[index]
            image = X_test[incorrect_index]
            label_data = y_test[incorrect_index]
            label_pred = majority_vote[incorrect_index]
            combined_label = f"{label_data}/{label_pred}"


            plt.subplot(n_rows, n_cols, index + 1)
            plt.imshow(image, cmap="binary", interpolation="nearest")
            plt.axis('off')
            plt.title(combined_label, fontsize=20)
        
plt.subplots_adjust(wspace=0.2, hspace=0.5)
plt.tight_layout()
save_fig('ensemble_error')
plt.show()
```